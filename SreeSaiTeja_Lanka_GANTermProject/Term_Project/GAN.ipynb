{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50, 28, 28, 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf #machine learning\n",
    "import numpy as np #matrix math\n",
    "import datetime #logging the time for model checkpoints and training\n",
    "import matplotlib.pyplot as plt #visualize results\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "#Step 1 - Collect dataset\n",
    "#MNIST - handwritten character digits ~50K training and validation images + labels, 10K testing\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#will ensure that the correct data has been downloaded to your \n",
    "#local training folder and then unpack that data to return a dictionary of DataSet instances.\n",
    "\n",
    "# get the current directory \n",
    "#current_directory = os.path.join(os.path.dirname('wallpaper_old'))\n",
    "wallpaper = []\n",
    "# create the path to the data directory within the current directory\n",
    "#data_directory = os.path.join(r\"C:\\Users\\lanka\\Desktop\\CSE583_PRML\\Term_Project\\wallpaper_old\\train\", \"CM\")\n",
    "paths  = pathlib.Path(r\"C:\\Users\\lanka\\Desktop\\CSE583_PRML\\Term_Project\\wallpaper_old\\train\\CM\").glob('*.png')\n",
    "paths_list =[x for x in paths]\n",
    "for i in paths_list:\n",
    "    wallpaper.append(np.array((Image.open(str(i))).resize((28, 28))).reshape(28, 28, 1))\n",
    "#wallpaper = input_data.read_data_sets(data_directory, one_hot = True) \n",
    "print(len(wallpaper[1]))\n",
    "np.shape(np.array(wallpaper[0:50]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-7b5eabc3fe6d>:12: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\lanka\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\lanka\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lanka\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lanka\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000001F42E693F28>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000001F42B3E2DA0>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000001F42B3E2D68>)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf #machine learning\n",
    "import numpy as np #matrix math\n",
    "import datetime #logging the time for model checkpoints and training\n",
    "import matplotlib.pyplot as plt #visualize results\n",
    "%matplotlib inline\n",
    "\n",
    "#Step 1 - Collect dataset\n",
    "#MNIST - handwritten character digits ~50K training and validation images + labels, 10K testing\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#will ensure that the correct data has been downloaded to your \n",
    "#local training folder and then unpack that data to return a dictionary of DataSet instances.\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")\n",
    "print(mnist)\n",
    "print(len(mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************ D I S C R I M I N A T O R function***********************\n",
    "\n",
    "\n",
    "def discriminator(x_image, reuse=False):\n",
    "    if (reuse):\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    # First convolutional and pool layers\n",
    "    # These search for 32 different 5 x 5 pixel features\n",
    "    #We’ll start off by passing the image through a convolutional layer. \n",
    "    #First, we create our weight and bias variables through tf.get_variable. \n",
    "    #Our first weight matrix (or filter) will be of size 5x5 and will have a output depth of 32. \n",
    "    #It will be randomly initialized from a normal distribution.\n",
    "    d_w1 = tf.get_variable('d_w1', [5, 5, 1, 32], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    #tf.constant_init generates tensors with constant values.\n",
    "    d_b1 = tf.get_variable('d_b1', [32], initializer=tf.constant_initializer(0))\n",
    "    #tf.nn.conv2d() is the Tensorflow’s function for a common convolution.\n",
    "    #It takes in 4 arguments. The first is the input volume (our 28 x 28 x 1 image in this case). \n",
    "    #The next argument is the filter/weight matrix. Finally, you can also change the stride and \n",
    "    #padding of the convolution. Those two values affect the dimensions of the output volume.\n",
    "    #\"SAME\" tries to pad evenly left and right, but if the amount of columns to be added is odd, \n",
    "    #it will add the extra column to the right,\n",
    "    #strides = [batch, height, width, channels]\n",
    "    d1 = tf.nn.conv2d(input = x_image, filter= d_w1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    #add the bias\n",
    "    d1 = d1 + d_b1\n",
    "    #squash with nonlinearity (ReLU)\n",
    "    d1 = tf.nn.relu(d1)\n",
    "    ##An average pooling layer performs down-sampling by dividing the input into \n",
    "    #rectangular pooling regions and computing the average of each region. \n",
    "    #It returns the averages for the pooling regions.\n",
    "    d1 = tf.nn.avg_pool(d1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    #As with any convolutional neural network, this module is repeated, \n",
    "    # Second convolutional and pool layers\n",
    "    # These search for 64 different 5 x 5 pixel features\n",
    "    d_w2 = tf.get_variable('d_w2', [5, 5, 32, 64], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    d_b2 = tf.get_variable('d_b2', [64], initializer=tf.constant_initializer(0))\n",
    "    d2 = tf.nn.conv2d(input=d1, filter=d_w2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    d2 = d2 + d_b2\n",
    "    d2 = tf.nn.relu(d2)\n",
    "    d2 = tf.nn.avg_pool(d2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "     #and then followed by a series of fully connected layers. \n",
    "    # First fully connected layer\n",
    "    d_w3 = tf.get_variable('d_w3', [7 * 7 * 64, 1024], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    d_b3 = tf.get_variable('d_b3', [1024], initializer=tf.constant_initializer(0))\n",
    "    d3 = tf.reshape(d2, [-1, 7 * 7 * 64])\n",
    "    d3 = tf.matmul(d3, d_w3)\n",
    "    d3 = d3 + d_b3\n",
    "    d3 = tf.nn.relu(d3)\n",
    "\n",
    "    #The last fully-connected layer holds the output, such as the class scores.\n",
    "    # Second fully connected layer\n",
    "    d_w4 = tf.get_variable('d_w4', [1024, 1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    d_b4 = tf.get_variable('d_b4', [1], initializer=tf.constant_initializer(0))\n",
    "\n",
    "    #At the end of the network, we do a final matrix multiply and \n",
    "    #return the activation value. \n",
    "    #For those of you comfortable with CNNs, this is just a simple binary classifier. Nothing fancy.\n",
    "    # Final layer\n",
    "    d4 = tf.matmul(d3, d_w4) + d_b4\n",
    "    # d4 dimensions: batch_size x 1\n",
    "\n",
    "    return d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************* G E N E R A T O R ***********************************\n",
    "\n",
    "\n",
    "#You can think of the generator as being a kind of reverse ConvNet. With CNNs, the goal is to \n",
    "#transform a 2 or 3 dimensional matrix of pixel values into a single probability. A generator, \n",
    "#however, seeks to take a d-dimensional noise vector and upsample it to become a 28 x 28 image. \n",
    "#ReLUs are then used to stabilize the outputs of each layer.\n",
    "#example of CNN blocks http://cs231n.github.io/convolutional-networks/#fc\n",
    "\n",
    "#it takes random inputs, and eventually mapping them down to a [1,28,28] pixel to match the MNIST data shape.  \n",
    "#Be begin by generating a dense 14×14 set of values, and then run through a handful of filters of\n",
    "#varying sizes and numbers of channels\n",
    "#weight matrices get progressively smaller\n",
    "\n",
    "def generator(batch_size, z_dim):\n",
    "    z = tf.truncated_normal([batch_size, z_dim], mean=0, stddev=1, name='z')\n",
    "    #first deconv block\n",
    "    g_w1 = tf.get_variable('g_w1', [z_dim, 3136], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b1 = tf.get_variable('g_b1', [3136], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g1 = tf.matmul(z, g_w1) + g_b1\n",
    "    g1 = tf.reshape(g1, [-1, 56, 56, 1])\n",
    "    g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='bn1')\n",
    "    g1 = tf.nn.relu(g1)\n",
    "\n",
    "    # Generate 50 features\n",
    "    g_w2 = tf.get_variable('g_w2', [3, 3, 1, z_dim/2], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b2 = tf.get_variable('g_b2', [z_dim/2], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g2 = tf.nn.conv2d(g1, g_w2, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g2 = g2 + g_b2\n",
    "    g2 = tf.contrib.layers.batch_norm(g2, epsilon=1e-5, scope='bn2')\n",
    "    g2 = tf.nn.relu(g2)\n",
    "    g2 = tf.image.resize_images(g2, [56, 56])\n",
    "\n",
    "    # Generate 25 features\n",
    "    g_w3 = tf.get_variable('g_w3', [3, 3, z_dim/2, z_dim/4], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b3 = tf.get_variable('g_b3', [z_dim/4], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g3 = tf.nn.conv2d(g2, g_w3, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g3 = g3 + g_b3\n",
    "    g3 = tf.contrib.layers.batch_norm(g3, epsilon=1e-5, scope='bn3')\n",
    "    g3 = tf.nn.relu(g3)\n",
    "    g3 = tf.image.resize_images(g3, [56, 56])\n",
    "\n",
    "    # Final convolution with one output channel\n",
    "    g_w4 = tf.get_variable('g_w4', [1, 1, z_dim/4, 1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b4 = tf.get_variable('g_b4', [1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g4 = tf.nn.conv2d(g3, g_w4, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g4 = g4 + g_b4\n",
    "    g4 = tf.sigmoid(g4)\n",
    "\n",
    "    # No batch normalization at the final layer, but we do add\n",
    "    # a sigmoid activator to make the generated images crisper.\n",
    "    # Dimensions of g4: batch_size x 28 x 28 x 1\n",
    "\n",
    "    return g4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "batch_size = 50\n",
    "z_dimensions = 100\n",
    "\n",
    "\n",
    "# x_placeholder is for feeding input images to the discriminator\n",
    "x_placeholder = tf.placeholder(dtype = \"float\", shape = [None,28,28, 1], name = 'x_placeholder')\n",
    "print(x_placeholder.shape)\n",
    "\n",
    "#One of the trickiest parts about understanding GANs is that the loss function is a little bit more complex than that\n",
    "#of a traditional CNN classifiers (For those, a simple MSE or Hinge Loss would do the trick). \n",
    "#If you think back to the introduction, a GAN can be thought of as a zero sum minimax game. \n",
    "#The generator is constantly improving to produce more and more realistic images, while the discriminator is \n",
    "#trying to get better and better at distinguishing between real and generated images.\n",
    "#This means that we need to formulate loss functions that affect both networks. \n",
    "#Let’s take a look at the inputs and outputs of our networks.\n",
    "\n",
    "Gz = generator(batch_size, z_dimensions)\n",
    "# Gz holds the generated images\n",
    "#g(z)\n",
    "\n",
    "Dx = discriminator(x_placeholder)\n",
    "# Dx hold the discriminator's prediction probabilities\n",
    "# for real MNIST images\n",
    "#d(x)\n",
    "\n",
    "Dg = discriminator(Gz, reuse=True)\n",
    "# Dg holds discriminator prediction probabilities for generated images\n",
    "#d(g(z))\n",
    "\n",
    "\n",
    "\n",
    "#So, let’s first think about what we want out of our networks. We want the generator network to create \n",
    "#images that will fool the discriminator. The generator wants the discriminator to output a 1 (positive example).\n",
    "#Therefore, we want to compute the loss between the Dg and label of 1. This can be done through \n",
    "#the tf.nn.sigmoid_cross_entropy_with_logits function. This means that the cross entropy loss will \n",
    "#be taken between the two arguments. The \"with_logits\" component means that the function will operate \n",
    "#on unscaled values. Basically, this means that instead of using a softmax function to squish the output\n",
    "#activations to probability values from 0 to 1, we simply return the unscaled value of the matrix multiplication.\n",
    "#Take a look at the last line of our discriminator. There's no softmax or sigmoid layer at the end.\n",
    "#The reduce mean function just takes the mean value of all of the components in the matrixx returned \n",
    "#by the cross entropy function. This is just a way of reducing the loss to a single scalar value, \n",
    "#instead of a vector or matrix.\n",
    "#https://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks\n",
    "\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg, labels=tf.ones_like(Dg)))\n",
    "\n",
    "\n",
    "#Now, let’s think about the discriminator’s point of view. Its goal is to just get the correct labels \n",
    "#(output 1 for each MNIST digit and 0 for the generated ones). We’d like to compute the loss between Dx \n",
    "#and the correct label of 1 as well as the loss between Dg and the correct label of 0.\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dx, labels=tf.fill([batch_size, 1], 0.9)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg, labels=tf.zeros_like(Dg)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in tvars if 'd_' in var.name]\n",
    "g_vars = [var for var in tvars if 'g_' in var.name]\n",
    "\n",
    "# Train the discriminator\n",
    "# Increasing from 0.001 in GitHub version\n",
    "#with tf.variable_scope(tf.get_variable_scope(), reuse=False) as scope:\n",
    "    #Next, we specify our two optimizers. In today’s era of deep learning, Adam seems to be the\n",
    "    #best SGD optimizer as it utilizes adaptive learning rates and momentum. \n",
    "    #We call Adam's minimize function and also specify the variables that we want it to update.\n",
    "with tf.variable_scope('fake-optimizer', reuse = tf.AUTO_REUSE):\n",
    "    d_trainer_fake = tf.train.AdamOptimizer(0.0001).minimize(d_loss_fake, var_list=d_vars)\n",
    "with tf.variable_scope('real-optimizer', reuse = tf.AUTO_REUSE):\n",
    "    d_trainer_real = tf.train.AdamOptimizer(0.0001).minimize(d_loss_real, var_list=d_vars)\n",
    "\n",
    "    # Train the generator\n",
    "    # Decreasing from 0.004 in GitHub version\n",
    "with tf.variable_scope('optimizer', reuse = tf.AUTO_REUSE):\n",
    "    g_trainer = tf.train.AdamOptimizer(0.0001).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard/gan/\n"
     ]
    }
   ],
   "source": [
    "#Outputs a Summary protocol buffer containing a single scalar value.\n",
    "tf.summary.scalar('Generator_loss', g_loss)\n",
    "tf.summary.scalar('Discriminator_loss_real', d_loss_real)\n",
    "tf.summary.scalar('Discriminator_loss_fake', d_loss_fake)\n",
    "\n",
    "d_real_count_ph = tf.placeholder(dtype = \"float\", shape = [None,28,28, 1], name = 'd_real_count_ph')\n",
    "d_fake_count_ph = tf.placeholder(dtype = \"float\", shape = [None,28,28, 1], name = 'd_fake_count_ph')\n",
    "g_count_ph = tf.placeholder(dtype = \"float\", shape = [None,28,28, 1], name = 'g_count_ph')\n",
    "\n",
    "tf.summary.scalar('d_real_count', d_real_count_ph)\n",
    "tf.summary.scalar('d_fake_count', d_fake_count_ph)\n",
    "tf.summary.scalar('g_count', g_count_ph)\n",
    "\n",
    "# Sanity check to see how the discriminator evaluates\n",
    "# generated and real MNIST images\n",
    "d_on_generated = tf.reduce_mean(discriminator(generator(batch_size, z_dimensions)))\n",
    "d_on_real = tf.reduce_mean(discriminator(x_placeholder))\n",
    "\n",
    "tf.summary.scalar('d_on_generated_eval', d_on_generated)\n",
    "tf.summary.scalar('d_on_real_eval', d_on_real)\n",
    "\n",
    "images_for_tensorboard = generator(batch_size, z_dimensions)\n",
    "tf.summary.image('Generated_images', images_for_tensorboard, 10)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/gan/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "print(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape () for Tensor 'd_real_count_ph_7:0', which has shape '(?, 28, 28, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-1a46f5dc3da6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m#real_image_batch = wallpaper.validation.next_batch(batch_size)[0].reshape([batch_size, 56, 56, 1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx_placeholder1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreal_image_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_real_count_ph\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0md_real_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_fake_count_ph\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0md_fake_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_count_ph\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mg_count\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1128\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1129\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape () for Tensor 'd_real_count_ph_7:0', which has shape '(?, 28, 28, 1)'"
     ]
    }
   ],
   "source": [
    "x_placeholder1 = tf.placeholder(dtype = \"float\", shape = [1,56,56, 1], name = 'x_placeholder')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#During every iteration, there will be two updates being made, one to the discriminator and one to the generator. \n",
    "#For the generator update, we’ll feed in a random z vector to the generator and pass that output to the discriminator\n",
    "#to obtain a probability score (this is the Dg variable we specified earlier).\n",
    "#As we remember from our loss function, the cross entropy loss gets minimized, \n",
    "#and only the generator’s weights and biases get updated.\n",
    "#We'll do the same for the discriminator update. We’ll be taking a batch of images \n",
    "#from the wallpaper variable we created way at the beginning of our program.\n",
    "#These will serve as the positive examples, while the images in the previous section are the negative ones.\n",
    "\n",
    "gLoss = 0\n",
    "dLossFake, dLossReal = 1, 1\n",
    "d_real_count, d_fake_count, g_count = 0, 0, 0\n",
    "no_of_batches = 1000/batch_size \n",
    "curr_batch= 0\n",
    "\n",
    "for i in range(int(no_of_batches)):\n",
    "    real_image_batch =np.array(wallpaper[curr_batch*50: (curr_batch+1)*50]).reshape([batch_size, 28, 28, 1])\n",
    "    curr_batch = curr_batch+1\n",
    "    if dLossFake > 0.6:\n",
    "        # Train discriminator on generated images\n",
    "        _, dLossReal, dLossFake, gLoss = sess.run([d_trainer_fake, d_loss_real, d_loss_fake, g_loss], {x_placeholder: real_image_batch})\n",
    "        d_fake_count += 1\n",
    "\n",
    "    if gLoss > 0.5:\n",
    "        # Train the generator\n",
    "        _, dLossReal, dLossFake, gLoss = sess.run([g_trainer, d_loss_real, d_loss_fake, g_loss], {x_placeholder: real_image_batch})\n",
    "        g_count += 1\n",
    "\n",
    "    if dLossReal > 0.45:\n",
    "        # If the discriminator classifies real images as fake,\n",
    "        # train discriminator on real values\n",
    "        _, dLossReal, dLossFake, gLoss = sess.run([d_trainer_real, d_loss_real, d_loss_fake, g_loss], {x_placeholder: real_image_batch})\n",
    "        d_real_count += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        image_batch = np.array(wallpaper[curr_batch*50: (curr_batch+1)*50])\n",
    "        real_image_batch = list()\n",
    "        \n",
    "    for i in range(curr_batch*50, (curr_batch+1)*50):\n",
    "        real_image_batch.append(np.array(Image.open(str(paths_list[i])).resize((56, 56))).reshape(56, 56, 1))\n",
    "        #real_image_batch.append(np.array(Image.open(str(paths_list[i])).resize((28, 28))).reshape( 28, 28, 1))\n",
    "        #wallpaper = input_data.read_data_sets(data_directory, one_hot = True) \n",
    "        real_image_batch = np.array(real_image_batch)\n",
    "        #print(len(real_image_batch))\n",
    "        \n",
    "        #real_image_batch = wallpaper.validation.next_batch(batch_size)[0].reshape([batch_size, 56, 56, 1])\n",
    "        summary = sess.run(merged, {x_placeholder1: real_image_batch, d_real_count_ph: d_real_count, d_fake_count_ph: d_fake_count, g_count_ph: g_count})\n",
    "        \n",
    "        writer.add_summary(summary, i)\n",
    "        d_real_count, d_fake_count, g_count = 0, 0, 0\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        # Periodically display a sample image in the notebook\n",
    "        # (These are also being sent to TensorBoard every 10 iterations)\n",
    "        images = sess.run(generator(3, z_dimensions)) \n",
    "        d_result = sess.run(discriminator(x_placeholder), {x_placeholder: images})\n",
    "        print(\"TRAINING STEP\", i, \"AT\", datetime.datetime.now())\n",
    "        for j in range(3):\n",
    "            print(\"Discriminator classification\", d_result[j])\n",
    "            im = images[j, :, :, 0]\n",
    "            plt.imshow(im.reshape([256, 256]), cmap='Greys')\n",
    "            plt.show()\n",
    "\n",
    "    if i % 5000 == 0:\n",
    "        save_path = saver.save(sess, \"models/pretrained_gan.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 56, 56, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_image_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0f1f750b52dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx_placeholder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mreal_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwallpaper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mreal_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx_placeholder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'validation'"
     ]
    }
   ],
   "source": [
    "test_images = sess.run(generator(10, 100))\n",
    "test_eval = sess.run(discriminator(x_placeholder), {x_placeholder: test_images})\n",
    "\n",
    "real_images = wallpaper.validation.next_batch(10)[0].reshape([10, 28, 28, 1])\n",
    "real_eval = sess.run(discriminator(x_placeholder), {x_placeholder: real_images})\n",
    "\n",
    "# Show discriminator's probabilities for the generated images,\n",
    "# and display the images\n",
    "for i in range(10):\n",
    "    print(test_eval[i])\n",
    "    plt.imshow(test_images[i, :, :, 0], cmap='Greys')\n",
    "    plt.show()\n",
    "\n",
    "# Now do the same for real WALLPAPER images\n",
    "for i in range(10):\n",
    "    print(real_eval[i])\n",
    "    plt.imshow(real_images[i, :, :, 0], cmap='Greys')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
